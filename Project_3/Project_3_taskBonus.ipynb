{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from numpy.polynomial.chebyshev import Chebyshev\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pde import CartesianGrid, ScalarField, solve_poisson_equation\n",
    "import copy \n",
    "import pandas as pd\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Construct the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_function(class_type, x_grid, coeff_range = (-3, 3), max_n_polynomals = 10):\n",
    "    if class_type == \"GP\":\n",
    "        kernel = RBF(length_scale=1.0)\n",
    "        gp = GaussianProcessRegressor(kernel=kernel, alpha=0.1, n_restarts_optimizer=10, normalize_y=True)\n",
    "        f = gp.sample_y(x_grid.reshape(-1, 1), 1).flatten()\n",
    "    elif class_type == \"PL\":\n",
    "        num_pieces = np.random.randint(\n",
    "            2, 10\n",
    "        ) \n",
    "        jumps = np.sort(np.random.choice(x_grid, num_pieces, replace=False))\n",
    "        slopes = np.random.uniform(*coeff_range, num_pieces - 1)\n",
    "        intercepts = np.random.uniform(*coeff_range, num_pieces - 1)\n",
    "\n",
    "        f = np.zeros_like(x_grid)\n",
    "        for i in range(1, len(jumps)):\n",
    "            mask = (x_grid >= jumps[i - 1]) & (x_grid <= jumps[i])\n",
    "            f[mask] = slopes[i - 1] * x_grid[mask] + intercepts[i - 1]\n",
    "    elif class_type == \"CP\":\n",
    "        # Chebyshev Polynomials\n",
    "        num_polynomials = np.random.randint(1, max_n_polynomals)\n",
    "        coeffs =  np.random.uniform(*coeff_range, num_polynomials)\n",
    "        cheb_poly = Chebyshev(coeffs, domain=[-1, 1])\n",
    "        f = cheb_poly(x)\n",
    "\n",
    "    f[0] = 0\n",
    "    f[-1] = 0\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_poisson(f, N):\n",
    "    \"\"\"\n",
    "    Solve the Poisson equation with the given function f and grid size N\n",
    "\n",
    "    Args:\n",
    "    f: function to solve for\n",
    "    N: grid size\n",
    "\n",
    "    Returns:\n",
    "    result: solution to the Poisson equation\n",
    "    \"\"\"\n",
    "    # Define the grid\n",
    "    grid = CartesianGrid([[-1, 1]], N)\n",
    "    # Defined the scalar field\n",
    "    field = ScalarField(grid, data=f)\n",
    "    # Define the PDE problem with the boundary conditions\n",
    "    result =  solve_poisson_equation(field, bc=({\"value\": 0}, {\"value\": 0}))\n",
    "\n",
    "    return result.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset\n",
    "N = 100\n",
    "x = np.linspace(-1, 1, N)\n",
    "\n",
    "D = {\"GP\": [], \"PL\": [], \"CP\": []}\n",
    "\n",
    "for classe in [\"GP\", \"PL\", \"CP\"]:\n",
    "    for i in range(100):\n",
    "        f = sample_function(classe, x)\n",
    "        u = solve_poisson(f, N)\n",
    "        D[classe].append((f, x, u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3, 100)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(D[\"GP\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(D, batch_size=16, split_ratio=0.8):\n",
    "\n",
    "    loaders  = {\"GP\": [], \"PL\": [], \"CP\": []}\n",
    "\n",
    "    for classe in [\"GP\", \"PL\", \"CP\"]:\n",
    "        data = np.array(D[classe])\n",
    "        inputs = torch.tensor(data[:, :2, :], dtype=torch.float32).to(device).permute(0, 2, 1)\n",
    "        outputs = torch.tensor(data[:, 2, :], dtype=torch.float32).to(device).unsqueeze(1)\n",
    "        torch_dataset = TensorDataset(inputs, outputs)\n",
    "        train_set, test_set = torch.utils.data.random_split(torch_dataset, [int(len(torch_dataset) * split_ratio), len(torch_dataset) - int(len(torch_dataset) * split_ratio)])\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "        loaders[classe] = (train_loader, test_loader)\n",
    "    \n",
    "    return loaders\n",
    "\n",
    "loaders = create_loaders(D, batch_size=32, split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Train Neural Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use FNO1d class from tutorial \n",
    "\n",
    "class SpectralConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1):\n",
    "        super(SpectralConv1d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        1D Fourier layer. It does FFT, linear transform, and Inverse FFT.\n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1\n",
    "\n",
    "        self.scale = 1 / (in_channels * out_channels)\n",
    "        self.weights1 = nn.Parameter(\n",
    "            self.scale\n",
    "            * torch.rand(in_channels, out_channels, self.modes1, dtype=torch.cfloat)\n",
    "        )\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul1d(self, input, weights):\n",
    "        # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n",
    "        return torch.einsum(\"bix,iox->box\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        # x.shape == [batch_size, in_channels, number of grid points]\n",
    "        # hint: use torch.fft library torch.fft.rfft\n",
    "        # use DFT to approximate the fourier transform\n",
    "\n",
    "        # Compute Fourier coefficients\n",
    "        x_ft = torch.fft.rfft(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(\n",
    "            batchsize,\n",
    "            self.out_channels,\n",
    "            x.size(-1) // 2 + 1,\n",
    "            device=x.device,\n",
    "            dtype=torch.cfloat,\n",
    "        )\n",
    "        out_ft[:, :, : self.modes1] = self.compl_mul1d(\n",
    "            x_ft[:, :, : self.modes1], self.weights1\n",
    "        )\n",
    "\n",
    "        # Return to physical space\n",
    "        x = torch.fft.irfft(out_ft, n=x.size(-1))\n",
    "        return x\n",
    "\n",
    "\n",
    "class FNO1d(nn.Module):\n",
    "    def __init__(self, modes, width):\n",
    "        super(FNO1d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "\n",
    "        input: the solution of the initial condition and location (a(x), x)\n",
    "        input shape: (batchsize, x=s, c=2)\n",
    "        output: the solution of a later timestep\n",
    "        output shape: (batchsize, x=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes\n",
    "        self.width = width\n",
    "        self.padding = 1  # pad the domain if input is non-periodic\n",
    "        self.linear_p = nn.Linear(\n",
    "            2, self.width\n",
    "        )  # input channel is 2: (u0(x), x) --> GRID IS INCLUDED!\n",
    "\n",
    "        self.spect1 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.spect2 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.spect3 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.lin0 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.lin1 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.lin2 = nn.Conv1d(self.width, self.width, 1)\n",
    "\n",
    "        self.linear_q = nn.Linear(self.width, 32)\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "\n",
    "        self.activation = torch.nn.Tanh()\n",
    "\n",
    "    def fourier_layer(self, x, spectral_layer, conv_layer):\n",
    "        return self.activation(spectral_layer(x) + conv_layer(x))\n",
    "\n",
    "    def linear_layer(self, x, linear_transformation):\n",
    "        return self.activation(linear_transformation(x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # grid = self.get_grid(x.shape, x.device)\n",
    "        # x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.linear_p(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # x = F.pad(x, [0, self.padding])  # pad the domain if input is non-periodic\n",
    "\n",
    "        x = self.fourier_layer(x, self.spect1, self.lin0)\n",
    "        x = self.fourier_layer(x, self.spect2, self.lin1)\n",
    "        x = self.fourier_layer(x, self.spect3, self.lin2)\n",
    "\n",
    "        # x = x[..., :-self.padding]  # pad the domain if input is non-periodic\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = self.linear_layer(x, self.linear_q)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.12207488715648651\n",
      "Epoch 10, loss: 0.055168223877747856\n",
      "Epoch 20, loss: 0.05417410532633463\n",
      "Epoch 30, loss: 0.0541255809366703\n",
      "Epoch 40, loss: 0.05412499109903971\n",
      "Epoch 50, loss: 0.05412482718626658\n",
      "Epoch 60, loss: 0.05412473653753599\n",
      "Epoch 70, loss: 0.05412467444936434\n",
      "Epoch 80, loss: 0.05412463843822479\n",
      "Epoch 90, loss: 0.05412461732824644\n",
      "Epoch 0, loss: 0.10641288260618846\n",
      "Epoch 10, loss: 0.03483750236531099\n",
      "Epoch 20, loss: 0.03074645499388377\n",
      "Epoch 30, loss: 0.03242842108011246\n",
      "Epoch 40, loss: 0.03232711801926295\n",
      "Epoch 50, loss: 0.03307214441398779\n",
      "Epoch 60, loss: 0.02976777528723081\n",
      "Epoch 70, loss: 0.03241293070216974\n",
      "Epoch 80, loss: 0.03163355775177479\n",
      "Epoch 90, loss: 0.029726338262359302\n",
      "Epoch 0, loss: 0.5194552143414816\n",
      "Epoch 10, loss: 0.13301007449626923\n",
      "Epoch 20, loss: 0.11464632550875346\n",
      "Epoch 30, loss: 0.10252510259548824\n",
      "Epoch 40, loss: 0.10898800194263458\n",
      "Epoch 50, loss: 0.10055247445901234\n",
      "Epoch 60, loss: 0.10451279083887736\n",
      "Epoch 70, loss: 0.10529660681883495\n",
      "Epoch 80, loss: 0.10185141116380692\n",
      "Epoch 90, loss: 0.10370837648709615\n"
     ]
    }
   ],
   "source": [
    "neural_operators = {\"GP\": FNO1d(16, 64), \"PL\": FNO1d(16, 64), \"CP\": FNO1d(16, 64)}\n",
    "\n",
    "l_r = 0.001\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "for classe in [\"GP\", \"PL\", \"CP\"]:\n",
    "    optimizer = optim.Adam(neural_operators[classe].parameters(), lr=l_r)\n",
    "    criterion = nn.MSELoss()\n",
    "    train_loader = loaders[classe][0]\n",
    "    neural_operators[classe].to(device)\n",
    "    neural_operators[classe].train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = neural_operators[classe](inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, loss: {running_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Evaluation: Zero-Shot and Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing model from GP and evaluating on GP\n",
      "Epoch 0, loss: 0.054124604910612106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lorenzotarricone/miniconda3/envs/torch_tutorial/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([20, 1, 100])) that is different to the input size (torch.Size([20, 100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, loss: 0.05575158819556236\n",
      "Epoch 10, loss: 0.054807908833026886\n",
      "Epoch 15, loss: 0.0542176216840744\n",
      "Epoch 20, loss: 0.05415672808885574\n",
      "Epoch 25, loss: 0.054184917360544205\n",
      "\n",
      "\n",
      "Comparing model from GP and evaluating on PL\n",
      "Epoch 0, loss: 0.16372349858283997\n",
      "Epoch 5, loss: 0.05774083361029625\n",
      "Epoch 10, loss: 0.051015228033065796\n",
      "Epoch 15, loss: 0.052244994789361954\n",
      "Epoch 20, loss: 0.04645328223705292\n",
      "Epoch 25, loss: 0.046484436839818954\n",
      "\n",
      "\n",
      "Comparing model from GP and evaluating on CP\n",
      "Epoch 0, loss: 0.34076589345932007\n",
      "Epoch 5, loss: 0.09462401270866394\n",
      "Epoch 10, loss: 0.08999630063772202\n",
      "Epoch 15, loss: 0.09238732606172562\n",
      "Epoch 20, loss: 0.08736167103052139\n",
      "Epoch 25, loss: 0.08164644241333008\n",
      "\n",
      "\n",
      "Comparing model from PL and evaluating on GP\n",
      "Epoch 0, loss: 0.061840325593948364\n",
      "Epoch 5, loss: 0.07124020904302597\n",
      "Epoch 10, loss: 0.063709557056427\n",
      "Epoch 15, loss: 0.05962558090686798\n",
      "Epoch 20, loss: 0.05594318360090256\n",
      "Epoch 25, loss: 0.05414467304944992\n",
      "\n",
      "\n",
      "Comparing model from PL and evaluating on PL\n",
      "Epoch 0, loss: 0.04038151726126671\n",
      "Epoch 5, loss: 0.050974760204553604\n",
      "Epoch 10, loss: 0.04205649346113205\n",
      "Epoch 15, loss: 0.04076024517416954\n",
      "Epoch 20, loss: 0.03939325734972954\n",
      "Epoch 25, loss: 0.038137830793857574\n",
      "\n",
      "\n",
      "Comparing model from PL and evaluating on CP\n",
      "Epoch 0, loss: 0.10279964655637741\n",
      "Epoch 5, loss: 0.10164850950241089\n",
      "Epoch 10, loss: 0.10074608027935028\n",
      "Epoch 15, loss: 0.09984634071588516\n",
      "Epoch 20, loss: 0.10029225796461105\n",
      "Epoch 25, loss: 0.09952420741319656\n",
      "\n",
      "\n",
      "Comparing model from CP and evaluating on GP\n",
      "Epoch 0, loss: 0.054190494120121\n",
      "Epoch 5, loss: 0.11041739583015442\n",
      "Epoch 10, loss: 0.07238546758890152\n",
      "Epoch 15, loss: 0.0606984905898571\n",
      "Epoch 20, loss: 0.05822097510099411\n",
      "Epoch 25, loss: 0.05668387562036514\n",
      "\n",
      "\n",
      "Comparing model from CP and evaluating on PL\n",
      "Epoch 0, loss: 0.043018922209739685\n",
      "Epoch 5, loss: 0.044934142380952835\n",
      "Epoch 10, loss: 0.03931361436843872\n",
      "Epoch 15, loss: 0.034813858568668365\n",
      "Epoch 20, loss: 0.03298697993159294\n",
      "Epoch 25, loss: 0.033128753304481506\n",
      "\n",
      "\n",
      "Comparing model from CP and evaluating on CP\n",
      "Epoch 0, loss: 0.05961973965167999\n",
      "Epoch 5, loss: 0.10915282368659973\n",
      "Epoch 10, loss: 0.0802663117647171\n",
      "Epoch 15, loss: 0.06342649459838867\n",
      "Epoch 20, loss: 0.06400427967309952\n",
      "Epoch 25, loss: 0.06246232986450195\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model trained on</th>\n",
       "      <th>Model evaluated on</th>\n",
       "      <th>Zero-shot loss</th>\n",
       "      <th>Finetuned loss</th>\n",
       "      <th>relative L2 error improvement (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>GP</td>\n",
       "      <td>GP</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>GP</td>\n",
       "      <td>PL</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.035</td>\n",
       "      <td>75.386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>GP</td>\n",
       "      <td>CP</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.093</td>\n",
       "      <td>74.673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>PL</td>\n",
       "      <td>GP</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.054</td>\n",
       "      <td>12.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>PL</td>\n",
       "      <td>PL</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>PL</td>\n",
       "      <td>CP</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>CP</td>\n",
       "      <td>GP</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-1.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>CP</td>\n",
       "      <td>PL</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.032</td>\n",
       "      <td>13.929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>CP</td>\n",
       "      <td>CP</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-3.465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       " Model trained on Model evaluated on Zero-shot loss Finetuned loss   \n",
       "               GP                 GP          0.054          0.054  \\\n",
       "               GP                 PL           0.14          0.035   \n",
       "               GP                 CP          0.369          0.093   \n",
       "               PL                 GP          0.062          0.054   \n",
       "               PL                 PL          0.032          0.032   \n",
       "               PL                 CP          0.082          0.082   \n",
       "               CP                 GP          0.054          0.055   \n",
       "               CP                 PL          0.037          0.032   \n",
       "               CP                 CP          0.075          0.078   \n",
       "\n",
       " relative L2 error improvement (%)  \n",
       "                            -0.154  \n",
       "                            75.386  \n",
       "                            74.673  \n",
       "                            12.034  \n",
       "                             0.576  \n",
       "                             0.138  \n",
       "                            -1.757  \n",
       "                            13.929  \n",
       "                            -3.465  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the array where to store the results: as many columns as possible combinations of\n",
    "# classes and three columns for the zero-shot loss, the finetuned loss and the improvement\n",
    "R = np.array([])\n",
    "epochs_finetuing = 30\n",
    "\n",
    "for i, classe1 in enumerate([\"GP\", \"PL\", \"CP\"]):\n",
    "    for j, classe2 in enumerate([\"GP\", \"PL\", \"CP\"]):\n",
    "        print(f\"Comparing model from {classe1} and evaluating on {classe2}\")\n",
    "        # do deep copy of the neural operator to avoid overwriting\n",
    "        model = copy.deepcopy(neural_operators[classe1])\n",
    "\n",
    "        ## Zero-shot loss ##\n",
    "        test_loader = loaders[classe2][1]\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        model.eval()\n",
    "        zero_shot_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader, 0):\n",
    "                inputs, labels = data\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels) \n",
    "                zero_shot_loss += loss.item()\n",
    "        zero_shot_loss /= len(test_loader)\n",
    "\n",
    "        # Finetuned loss\n",
    "        optimizer = optim.Adam(model.parameters(), lr=l_r)\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        # get 20 random samples from the test loader\n",
    "        test_loader_finetune = DataLoader(\n",
    "            Subset(test_loader.dataset, np.random.choice(len(test_loader.dataset), 20)),\n",
    "            batch_size=20,\n",
    "        )\n",
    "\n",
    "        for epoch in range(epochs_finetuing):\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(test_loader_finetune, 0):\n",
    "                inputs, labels = data\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"Epoch {epoch}, loss: {running_loss / len(test_loader_finetune)}\")\n",
    "\n",
    "        model.eval()\n",
    "        finetuned_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader, 0):\n",
    "                inputs, labels = data\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                finetuned_loss += loss.item()\n",
    "        finetuned_loss /= len(test_loader)\n",
    "\n",
    "        # stack the results in the array\n",
    "        R = np.append(R, np.array([classe1, classe2, round(zero_shot_loss, 3), round(finetuned_loss, 3), round(100 * (zero_shot_loss - finetuned_loss) /zero_shot_loss, 3)]))\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "columns = [\n",
    "    \"Model trained on\",\n",
    "    \"Model evaluated on\",\n",
    "    \"Zero-shot loss\",\n",
    "    \"Finetuned loss\",\n",
    "    \"relative L2 error improvement (%)\",\n",
    "]\n",
    "df = pd.DataFrame(R.reshape(9,5), columns=columns)\n",
    "blankIndex = [\"\"] * len(df)\n",
    "df.index = blankIndex\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "df.to_csv(\"results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      "Model trained on & Model evaluated on & Zero-shot loss & Finetuned loss & relative L2 error improvement (%) \\\\\n",
      "\\midrule\n",
      "GP & GP & 0.054 & 0.054 & -0.154 \\\\\n",
      "GP & PL & 0.14 & 0.035 & 75.386 \\\\\n",
      "GP & CP & 0.369 & 0.093 & 74.673 \\\\\n",
      "PL & GP & 0.062 & 0.054 & 12.034 \\\\\n",
      "PL & PL & 0.032 & 0.032 & 0.576 \\\\\n",
      "PL & CP & 0.082 & 0.082 & 0.138 \\\\\n",
      "CP & GP & 0.054 & 0.055 & -1.757 \\\\\n",
      "CP & PL & 0.037 & 0.032 & 13.929 \\\\\n",
      "CP & CP & 0.075 & 0.078 & -3.465 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to insert in the report \n",
    "print(df.to_latex(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
